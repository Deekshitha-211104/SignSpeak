{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b096aa2-e785-45e9-bd85-70d4101b15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pickle\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acb9fed-610a-4f32-ade6-3c9ec913d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU model definition (same as trained model)\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_size=256, num_layers=2, num_classes=2000):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)\n",
    "        last_out = gru_out[:, -1, :]\n",
    "        logits = self.fc(last_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d2aa66-ec46-4d6d-9e47-848e81d23a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0272172d-03e1-4b05-ae9e-6c6cc9c927d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeks_w4ub1k8\\Desktop\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\deeks_w4ub1k8\\Desktop\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load ResNet18 feature extractor (remove last FC layer)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "resnet.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d410d10c-76c6-4cda-ae88-4a99f0bcef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for ResNet input\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "191304a3-8966-4398-b11c-56627ff85373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resnet_feature(frame):\n",
    "    # Convert frame to PIL Image and preprocess\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img = img.resize((224, 224))  # Resize explicitly to 224x224\n",
    "    img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feature = resnet(img_tensor)  # Output shape: (1, 512, 1, 1)\n",
    "    feature = feature.view(-1).cpu()  # Flatten to (512,)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18bb9301-033d-4a75-822e-44f9e9ba81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sign_from_video(video_path, model_path, label_map, num_frames=10):\n",
    "    # Load your pretrained GRU model\n",
    "    model = GRUClassifier(input_size=512)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device).eval()\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"❌ Could not open video {video_path}\")\n",
    "        return \"Error\"\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count < num_frames:\n",
    "        print(f\"⚠️ Video too short ({frame_count} frames), requires at least {num_frames}.\")\n",
    "        cap.release()\n",
    "        return \"Too short\"\n",
    "    # Calculate evenly spaced frame indices (skip frame 0)\n",
    "    frame_indices = [int(frame_count * i / num_frames) for i in range(1, num_frames + 1)]\n",
    "\n",
    "    features_sequence = []\n",
    "\n",
    "    for idx, frame_idx in enumerate(frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Warning: couldn't read frame {frame_idx}\")\n",
    "            continue\n",
    "        feature = extract_resnet_feature(frame)\n",
    "        features_sequence.append(feature)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(features_sequence) == 0:\n",
    "        print(\"❌ No frames processed.\")\n",
    "        return \"No frames\"\n",
    "\n",
    "    # Stack features into tensor [num_frames, 512]\n",
    "    features_sequence = torch.stack(features_sequence)\n",
    "\n",
    "    # Add batch dimension: [1, num_frames, 512]\n",
    "    sequence = features_sequence.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequence)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "    # Map predicted class index to label\n",
    "    return label_map.get(predicted.item(), \"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57f326cf-276a-4a97-b032-8e9386a29850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_label(label):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(f\"The sign is {label}\")\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "165a90c2-e300-4c26-9367-f43d55af9796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeks_w4ub1k8\\AppData\\Local\\Temp\\ipykernel_24600\\3548675378.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: couldn't read frame 51\n",
      "🔤 Predicted Sign Label: delicious\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths — update as needed\n",
    "    video_path = \"C:\\\\Users\\\\deeks_w4ub1k8\\\\Downloads\\\\Testing _videos\\\\delicious.mp4\"\n",
    "    model_path = \"gru_classifier_model.pth\"\n",
    "    label_map_path = \"label_map.pkl\"\n",
    "\n",
    "    # Load label map (invert if it's {label: index})\n",
    "    with open(label_map_path, \"rb\") as f:\n",
    "        label_map = pickle.load(f)\n",
    "    if isinstance(next(iter(label_map.keys())), str):\n",
    "        label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "    # Predict and speak\n",
    "    predicted_label = predict_sign_from_video(video_path, model_path, label_map)\n",
    "    print(\"🔤 Predicted Sign Label:\", predicted_label)\n",
    "    speak_label(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcacb1-c69b-400d-b259-5de56bc36363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
